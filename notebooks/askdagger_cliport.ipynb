{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCcfRsHvOXRr"
   },
   "source": [
    "# **ASkDAgger**: Active Skill-level Data Aggregation for Interactive Imitation Learning\n",
    "\n",
    "This Colab tutorial is a code example for the paper *ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning*.\n",
    "This code example will show how we can train a robot novice to perform a pick-and-place task using ASkDAgger.\n",
    "\n",
    "### Overview of ASkDAgger\n",
    "\n",
    "<img src=../figures/askdagger_overview.png width=\"50%\">\n",
    "\n",
    "**Figure 1**: The Active Skill-level Data Aggregation (ASkDAgger) framework consists of three main components: S-Aware Gating (SAG), Foresight Interactive Experience Replay (FIER), and Prioritized Interactive Experience Replay (PIER).\n",
    "In this interactive imitation learning framework, we allow the novice to say: \"*I plan to do this, but I am uncertain.*\"\n",
    "The uncertainty gating threshold is set by SAG to track a user-specified metric: sensitivity, specificity, or minimum system success rate.\n",
    "Teacher feedback is obtained with FIER, enabling demonstrations through validation, relabeling, or teacher demonstrations.\n",
    "Lastly, PIER prioritizes replay based on novice success, uncertainty, and demonstration age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LSxYC9JE5Wkl",
    "outputId": "bb76ff6b-7bfe-405e-a3cd-d22249ed2ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jelle/askdagger_cliport/src/askdagger_cliport/environments/assets/\n",
      "Save path for results: /home/jelle/askdagger_cliport/exps_notebook/put-block-in-bowl-seen-colors-cliport-n0-train/0/interactive_training/fier_True_pier_True_sensitivity_0.9\n",
      "Loading from scratch.\n",
      "Training interrupted. Returning to seed 10.\n",
      "Training interrupted. Returning to seed 8.\n",
      "Training interrupted. Returning to seed 6.\n",
      "Training interrupted. Returning to seed 4.\n",
      "Training interrupted. Returning to seed 2.\n",
      "Training interrupted. Returning to seed 0.\n",
      "Start interactive training. Seed: -2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jelle/askdagger_cliport/.venv/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "import askdagger_cliport\n",
    "from askdagger_cliport.utils import utils\n",
    "from askdagger_cliport.interactive_agent import InteractiveAgent\n",
    "from askdagger_cliport import tasks\n",
    "from askdagger_cliport.environments.environment import Environment\n",
    "from askdagger_cliport.train_interactive import collect_demo\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from hydra.experimental import compose, initialize\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "\n",
    "gpu = \"[0]\"\n",
    "if not torch.cuda.is_available():\n",
    "  gpu = 0\n",
    "  warnings.warn(\"Not using GPU runtime! Training on CPU is very slow.\")\n",
    "\n",
    "task_name = \"put-block-in-bowl-seen-colors\"\n",
    "uncertainty_measure = \"entropy\"\n",
    "validation_demos = \"True\"\n",
    "FIER = \"True\"\n",
    "PIER = \"True\"\n",
    "n_demonstrations = 10\n",
    "relabeling_demos = FIER\n",
    "pier = PIER\n",
    "\n",
    "root_dir = Path(askdagger_cliport.__file__).resolve().parents[1]\n",
    "if os.path.exists(root_dir / \"exps_notebook\"):\n",
    "    shutil.rmtree(root_dir / \"exps_notebook\")\n",
    "import os\n",
    "with initialize(config_path=\"../src/askdagger_cliport/cfg\"):\n",
    "    icfg = compose(\n",
    "        config_name=\"train_interactive\",\n",
    "        overrides=[\n",
    "            # f\"root_dir={root_dir}/demo\",\n",
    "            \"train_interactive.log=False\",\n",
    "            f\"train_interactive.pier={pier}\",\n",
    "            f\"train_interactive.measure={uncertainty_measure}\",\n",
    "            f\"train_interactive.gpu={gpu}\",\n",
    "            \"train_interactive.batch_size=1\",\n",
    "            f\"validation_demos={validation_demos}\",\n",
    "            f\"relabeling_demos={relabeling_demos}\",\n",
    "            \"train_demos=0\",\n",
    "            f\"interactive_demos={n_demonstrations}\",\n",
    "            \"save_model=True\",\n",
    "            \"save_results=True\",\n",
    "            f\"model_task={task_name}\",\n",
    "            f\"train_interactive_task={task_name}\",\n",
    "            \"disp=False\",\n",
    "            \"exp_folder=exps_notebook\",\n",
    "        ],\n",
    "    )\n",
    "    print(icfg[\"assets_root\"])\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = Environment(icfg[\"assets_root\"], disp=False, shared_memory=False, hz=480)\n",
    "task = tasks.names[task_name]()\n",
    "env.set_task(task)\n",
    "interactive_agent = InteractiveAgent(icfg, mode=\"train\")\n",
    "seed = interactive_agent.seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNN4LwdKGgC9"
   },
   "source": [
    "# Train novice interactively using ASkDAgger\n",
    "\n",
    "Now we are ready to train! The training procedure is summarized below in Algorithm 1.\n",
    "At the beginning of each episode, we reset the environment (line 3).\n",
    "At each step of the episode, the novice receives a language command (which is printed) and observes the environment in the form of RGB-D data.\n",
    "The novice infers its policy (line 5) and we visualize the planned novice actions.\n",
    "The novice quantifies the related uncertainty (line 6).\n",
    "This uncertainty is used to perform gating between the novice and the teacher.\n",
    "SAG sets the gating threshold to track a desired sensitivity level (line 7), in this case, 0.9.\n",
    "If the uncertainty exceeds a threshold set SAG or in case of a random query with probability $p_\\mathrm{rand}=0.2$, the teacher is queried (line 10).\n",
    "We print the uncertainty level, current threshold, and whether the teacher is queried.\n",
    "If queried, the teacher can relabel goal for the novice's planned actions if they are valid for some other language command (line 10).\n",
    "Also, if the novice's plan is valid, the teacher can validate it and it will be aggregated into the dataset (line 10).\n",
    "Demonstrations are also visualized if given.\n",
    "Finally, we visualize the environment after an action is executed.\n",
    "At the end of each episode, we aggregate the demonstrations into the dataset (line 15) and update the model (line 17).\n",
    "\n",
    "<img src='../figures/alg.png' width=\"75%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RiQjH0td5xWB",
    "outputId": "0e307cde-f1a2-4417-a6b7-97a747c5ed98"
   },
   "outputs": [],
   "source": [
    "while interactive_agent.n_interactive < n_demonstrations:\n",
    "    seed += 2\n",
    "    interactive_agent.seed = seed\n",
    "\n",
    "    # Set seeds.\n",
    "    utils.set_seed(seed)\n",
    "    print(\"\\n-------------------------------------------------\\nInteractive demos: {}/{} | Seed: {}\".format(interactive_agent.n_interactive, icfg[\"interactive_demos\"], seed))\n",
    "\n",
    "    task.mode = \"train\"\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    D_i = []\n",
    "    reward = 0\n",
    "    for _ in range(task.max_steps):\n",
    "        lang_goal = info[\"lang_goal\"]\n",
    "        print(f\"Lang Goal: {lang_goal}\")\n",
    "\n",
    "        # Get agent action.\n",
    "        if len(obs[\"color\"]) == 0:\n",
    "            obs = env.get_obs()  # Only rendering when the agent acts, since rendering is slow.\n",
    "        agent_act, query = interactive_agent.act(obs, info)\n",
    "\n",
    "        # Visualize the novice's planned action\n",
    "        obs_copy = deepcopy(obs)\n",
    "        info_copy = deepcopy(info)\n",
    "        img, projected_img = interactive_agent.visualize_action(agent_act, obs_copy, info_copy)\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        ax[0].imshow(img)\n",
    "        ax[0].axis(\"off\")\n",
    "        ax[0].set_title(\"Novice Plan\")\n",
    "        ax[1].imshow(projected_img)\n",
    "        ax[1].axis(\"off\")\n",
    "        ax[1].set_title(\"Top View\")\n",
    "        plt.show()\n",
    "\n",
    "        # Query oracle if uncertainty is high.\n",
    "        env, obs, reward, terminated, truncated, info, demo = collect_demo(\n",
    "            obs=obs,\n",
    "            reward=reward,\n",
    "            info=info,\n",
    "            env=env,\n",
    "            agent_act=agent_act,\n",
    "            query=query,\n",
    "            interactive_agent=interactive_agent,\n",
    "            relabeling_demos=relabeling_demos,\n",
    "            validation_demos=validation_demos,\n",
    "            p_rand=icfg[\"train_interactive\"][\"p_rand\"],\n",
    "        )\n",
    "        done = terminated or truncated\n",
    "        interactive_agent.update_stats(demo)\n",
    "\n",
    "        # Rewards are privileged information, so we update it separately.\n",
    "        # We only use this information for logging purposes\n",
    "        interactive_agent.update_rewards(done, novice_reward=demo[\"novice_reward\"], system_reward=reward)\n",
    "\n",
    "        if demo[\"demo\"] is not None:\n",
    "            D_i.append(demo[\"demo\"])\n",
    "            if demo[\"oracle_demo\"]:\n",
    "                act = demo[\"demo\"][1]\n",
    "\n",
    "                # Visualize demonstration\n",
    "                img, projected_img = interactive_agent.visualize_action(act, obs_copy, info_copy)\n",
    "                fig, ax = plt.subplots(1, 2)\n",
    "                ax[0].imshow(img)\n",
    "                ax[0].axis(\"off\")\n",
    "                ax[0].set_title(\"Demonstration\")\n",
    "                ax[1].imshow(projected_img)\n",
    "                ax[1].axis(\"off\")\n",
    "                ax[1].set_title(\"Top View\")\n",
    "                plt.show()\n",
    "\n",
    "        if demo[\"relabeling_demo\"] is not None:\n",
    "            D_i.append(demo[\"relabeling_demo\"])\n",
    "\n",
    "        # Visualize environment after taking the action\n",
    "        plt.title(\"Result\")\n",
    "        plt.imshow(env.render())\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if len(D_i) > 0:\n",
    "        if len(obs[\"color\"]) == 0:\n",
    "            obs = interactive_agent.get_image(env.get_obs())\n",
    "        D_i.append((obs, None, reward, info))\n",
    "        interactive_agent.add_demo(seed, D_i)\n",
    "        interactive_agent.prioritize_replay()\n",
    "    interactive_agent.update_model()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
